{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a function to calculate word error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer1(ref, hyp ,debug=False):\n",
    "    ref=ref.lower()\n",
    "    hyp=hyp.lower()\n",
    "    r = ref.split()\n",
    "    h = hyp.split()\n",
    "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    # backtrace will hold the operations we've done.\n",
    "    # so we could later backtrace, like the WER algorithm requires us to.\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "\n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "\n",
    "    DEL_PENALTY=1 # Tact\n",
    "    INS_PENALTY=1 # Tact\n",
    "    SUB_PENALTY=1 # Tact\n",
    "    # First column represents the case where we achieve zero\n",
    "    # hypothesis words by deleting all reference words.\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "\n",
    "    # First row represents the case where we achieve the hypothesis\n",
    "    # by inserting all hypothesis words into a zero-length reference.\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "\n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "\n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    if debug:\n",
    "        print(\"OP\\tREF\\tHYP\")\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\".\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    if debug:\n",
    "        lines = list(reversed(lines))\n",
    "        for i,line in enumerate(lines):\n",
    "            if(line.startswith('.')):\n",
    "                if(i-1 > 0):\n",
    "                    if(not lines[i-1].startswith('.')):\n",
    "                        print(line)\n",
    "                        continue\n",
    "                if(i+1 < len(lines)):\n",
    "                    if(lines[i+1].startswith('.')):continue\n",
    "            print(line)\n",
    "        print(\"Ncor \" + str(numCor))\n",
    "        print(\"Nsub \" + str(numSub))\n",
    "        print(\"Ndel \" + str(numDel))\n",
    "        print(\"Nins \" + str(numIns))\n",
    "    return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "    return {'WER':wer_result, 'Cor':numCor, 'Sub':numSub, 'Ins':numIns, 'Del':numDel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to normalize our string lemmatization, case converison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatized(para):\n",
    "    para=para.lower()\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final=''\n",
    "    sent_text = nltk.sent_tokenize(para) \n",
    "    for sentence in sent_text:\n",
    "        x=([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "        x=\" \".join(x)+' '\n",
    "        final=final+x\n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the azure speech services , enter your subscription key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import wave\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# Set up the subscription info for the Speech Service:\n",
    "# Replace with your own subscription key and service region (e.g., \"westus\").\n",
    "\n",
    "speech_key, service_region = \"subscription_key\", \"westus\"\n",
    "\n",
    "def speech_recognize_continuous_from_file(weatherfilename):\n",
    "    \"\"\"performs continuous speech recognition with input from an audio file\"\"\"\n",
    "    # <SpeechContinuousRecognitionWithFile>\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region,speech_recognition_language='en-us')\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=weatherfilename)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        \"\"\"callback that stops continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    speech_recognizer.recognized.connect(lambda evt: obj1.append(evt) )\n",
    "#     # Connect callbacks to the events fired by the speech recognizer , comment out these if you don't want status updates\n",
    "    speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "    speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "    # stop continuous recognition on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "    # </SpeechContinuousRecognitionWithFile>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from jiwer import wer\n",
    "import numpy as np\n",
    "transcripts=[]\n",
    "#enter path for audio files to be tested\n",
    "path='C:\\\\Users\\\\eklavya\\\\istem\\\\test\\\\indian\\\\New Folder\\\\'\n",
    "#enter name of audio files to be tested\n",
    "vids=['Kiran_Bedi_How_I_remade_one_of_Indias_toughest_prisonst0',\n",
    "      'A_well_educated_mind_vs_a_well_formed_mind_Dr_Shashi_Tharoor_at_TEDxGateway_2013t1',\n",
    "      \"Thoughts_on_humanity_fame_and_love_Shah_Rukh_Khant1\",\n",
    "      \"What_makes_life_complete_Gaur_Gopal_Das_TEDxMITPt4\",\n",
    "      \"Why_is_India_so_filthy_The_Ugly_Indian_TEDxBangaloret3\" \n",
    "     ]\n",
    "for vid in vids:\n",
    "    print(vid)\n",
    "    obj1=[]\n",
    "    speech_recognize_continuous_from_file(path+vid+'.wav')\n",
    "    st=''\n",
    "    for i in obj1:\n",
    "        st=st+' '+((i.result).text)\n",
    "    transcripts.append(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing the results word error rate - removing punctuation marks and lemmatizatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ground_truths=[]\n",
    "for file in vids:\n",
    "    f= open((file[:-2]+'_transcribed.txt'),'r')\n",
    "    w=f.readlines()\n",
    "    ground_truths.append(w[int(file[-1])])\n",
    "wers=[]\n",
    "\n",
    "for i in range(len(transcripts)):\n",
    "    ground=(ground_truths[i].split('\\t')[1])\n",
    "    print('\\n',(ground),'\\n') # printing the actual ground truth without any normalization\n",
    "    print('\\n',(transcripts[i]),'\\n') # printing the actual transcripts without any normalization\n",
    "    ground=lemmatized(ground)\n",
    "    ground=\" \".join(re.findall(\"[a-zA-Z0-9]+\", ground)) # normalizing - removing punctuations\n",
    "    ele=\" \".join(re.findall(\"[a-zA-Z0-9]+\",(lemmatized(transcripts[i])))) \n",
    "    wer1(ground,ele,True)#  comment this line if you only want the word error rate and not the debugging\n",
    "    wers.append(wer(ground,ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(vids)):\n",
    "    print(vids[i],'\\t',wers[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
